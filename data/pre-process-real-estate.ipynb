{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Name of CSV file\n",
    "csv_filename = \"real-estate-listings.csv\" \n",
    "\n",
    "# Combine the current directory path with the CSV file name to get the full file path\n",
    "dataset_path = os.path.join(current_directory, csv_filename)\n",
    "\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'id' column\n",
    "df = df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     status  bed  bath  acre_lot        city        state  zip_code  \\\n",
      "0  for_sale    3     2      0.12    Adjuntas  Puerto Rico       601   \n",
      "1  for_sale    4     2      0.08    Adjuntas  Puerto Rico       601   \n",
      "2  for_sale    2     1      0.15  Juana Diaz  Puerto Rico       795   \n",
      "3  for_sale    4     2      0.10       Ponce  Puerto Rico       731   \n",
      "4  for_sale    6     2      0.05    Mayaguez  Puerto Rico       680   \n",
      "\n",
      "   house_size prev_sold_date   price  \n",
      "0         920            NaN  105000  \n",
      "1        1527            NaN   80000  \n",
      "2         748            NaN   67000  \n",
      "3        1800            NaN  145000  \n",
      "4           0            NaN   65000  \n"
     ]
    }
   ],
   "source": [
    "# List of columns to convert to integers\n",
    "columns_to_convert = ['bed', 'bath', 'zip_code', 'house_size', 'price']\n",
    "\n",
    "# Convert specified columns to integer, handling non-null values correctly\n",
    "for column in columns_to_convert:\n",
    "    # Remove decimals by converting to integers for non-null values\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'price' is zero (which originally were NaNs and got converted to zero)\n",
    "df = df[df['price'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 3452504\n",
      "Number of rows after: 2800402\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows before cleaning\n",
    "print(f\"Original number of rows: {len(df)}\")\n",
    "\n",
    "# Remove rows \n",
    "df = df.dropna(subset=['city', 'state', 'acre_lot'])\n",
    "\n",
    "# Print the number of rows after cleaning\n",
    "print(f\"Number of rows after: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after: 1900994\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'house_size' is 0\n",
    "df = df[df['house_size'] != 0]\n",
    "\n",
    "# Print the number of rows after cleaning\n",
    "print(f\"Number of rows after: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after: 1853038\n",
      "     status  bed  bath  acre_lot           city        state  zip_code  \\\n",
      "0  for_sale    3     2      0.12       Adjuntas  Puerto Rico       601   \n",
      "1  for_sale    4     2      0.08       Adjuntas  Puerto Rico       601   \n",
      "2  for_sale    2     1      0.15     Juana Diaz  Puerto Rico       795   \n",
      "3  for_sale    4     2      0.10          Ponce  Puerto Rico       731   \n",
      "5  for_sale    4     3      0.46  San Sebastian  Puerto Rico       612   \n",
      "\n",
      "   house_size prev_sold_date   price  \n",
      "0         920            NaN  105000  \n",
      "1        1527            NaN   80000  \n",
      "2         748            NaN   67000  \n",
      "3        1800            NaN  145000  \n",
      "5        2520            NaN  179000  \n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'zip_code' or 'bed' or 'bath' is 0\n",
    "df = df[(df['zip_code'] != 0) & (df['bath'] != 0) & (df['bed'] != 0)]\n",
    "\n",
    "# Print the number of rows after cleaning\n",
    "print(f\"Number of rows after: {len(df)}\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bed  bath  acre_lot           city        state  zip_code  house_size  \\\n",
      "0    3     2      0.12       Adjuntas  Puerto Rico       601         920   \n",
      "1    4     2      0.08       Adjuntas  Puerto Rico       601        1527   \n",
      "2    2     1      0.15     Juana Diaz  Puerto Rico       795         748   \n",
      "3    4     2      0.10          Ponce  Puerto Rico       731        1800   \n",
      "5    4     3      0.46  San Sebastian  Puerto Rico       612        2520   \n",
      "\n",
      "    price  \n",
      "0  105000  \n",
      "1   80000  \n",
      "2   67000  \n",
      "3  145000  \n",
      "5  179000  \n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['status', 'prev_sold_date'])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   bed  bath  acre_lot           city        state  zip_code  house_size  \\\n",
      "0    3     2      0.12       adjuntas  puerto rico       601         920   \n",
      "1    4     2      0.08       adjuntas  puerto rico       601        1527   \n",
      "2    2     1      0.15     juana diaz  puerto rico       795         748   \n",
      "3    4     2      0.10          ponce  puerto rico       731        1800   \n",
      "5    4     3      0.46  san sebastian  puerto rico       612        2520   \n",
      "\n",
      "    price  \n",
      "0  105000  \n",
      "1   80000  \n",
      "2   67000  \n",
      "3  145000  \n",
      "5  179000  \n"
     ]
    }
   ],
   "source": [
    "# Convert 'city' and 'state' columns to lowercase\n",
    "df['city'] = df['city'].str.lower()\n",
    "df['state'] = df['state'].str.lower()\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 1804425\n",
      "Number of rows after filtering: 1804425\n",
      "       bed  bath  acre_lot    city          state  zip_code  house_size  \\\n",
      "24201    2     1      0.34  agawam  massachusetts      1001         676   \n",
      "24206    3     1      0.46  agawam  massachusetts      1001        1196   \n",
      "24207    3     3      0.45  agawam  massachusetts      1001        2314   \n",
      "24208    3     2      0.36  agawam  massachusetts      1001        1276   \n",
      "24211    4     2      0.11  agawam  massachusetts      1001        1732   \n",
      "\n",
      "        price  \n",
      "24201  180000  \n",
      "24206  239900  \n",
      "24207  525000  \n",
      "24208  289900  \n",
      "24211  275000  \n"
     ]
    }
   ],
   "source": [
    "# State abbreviations to full names mapping (Only full names are used here)\n",
    "valid_states = {\n",
    "    'alabama', 'alaska', 'arizona', 'arkansas', 'california',\n",
    "    'colorado', 'connecticut', 'delaware', 'florida', 'georgia',\n",
    "    'hawaii', 'idaho', 'illinois', 'indiana', 'iowa',\n",
    "    'kansas', 'kentucky', 'louisiana', 'maine', 'maryland',\n",
    "    'massachusetts', 'michigan', 'minnesota', 'mississippi',\n",
    "    'missouri', 'montana', 'nebraska', 'nevada', 'new hampshire',\n",
    "    'new jersey', 'new mexico', 'new york', 'north carolina',\n",
    "    'north dakota', 'ohio', 'oklahoma', 'oregon', 'pennsylvania',\n",
    "    'rhode island', 'south carolina', 'south dakota', 'tennessee',\n",
    "    'texas', 'utah', 'vermont', 'virginia', 'washington',\n",
    "    'west virginia', 'wisconsin', 'wyoming'\n",
    "}\n",
    "\n",
    "# Filter the DataFrame to include only rows where the state is in the set of valid states\n",
    "df = df[df['state'].isin(valid_states)]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a new CSV file\n",
    "df.to_csv('clean-real-estate-listings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique city, state pairs in real estate listings that are not in the city table:\n",
      "                      city          state\n",
      "9                   Pelham  Massachusetts\n",
      "40                  Hadley  Massachusetts\n",
      "117       West Springfield  Massachusetts\n",
      "289             Willington    Connecticut\n",
      "294           East Windsor    Connecticut\n",
      "...                    ...            ...\n",
      "1798777          Belleview        Florida\n",
      "1798779         Fort McCoy        Florida\n",
      "1798917  Wilbur By the Sea        Florida\n",
      "1799631        Deer Island        Florida\n",
      "1800548           Ferndale        Florida\n",
      "\n",
      "[1286 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the real estate listings data\n",
    "real_estate_df = pd.read_csv('clean-real-estate-listings.csv')\n",
    "# Load the city table data\n",
    "city_df = pd.read_csv('city-table.csv')\n",
    "\n",
    "# Convert city_df to a set of tuples for faster lookup\n",
    "city_set = set(zip(city_df['city'], city_df['state']))\n",
    "\n",
    "# Find entries in real_estate_df that are not in city_set\n",
    "unique_cities = real_estate_df[~real_estate_df.apply(lambda row: (row['city'], row['state']) in city_set, axis=1)]\n",
    "\n",
    "# Display unique cities and states\n",
    "print(\"Unique city, state pairs in real estate listings that are not in the city table:\")\n",
    "print(unique_cities[['city', 'state']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Some entries did not get a city_id. Review the unmatched city and state names.\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Name of CSV file\n",
    "csv_filename = \"clean-real-estate-listings.csv\" \n",
    "\n",
    "# Combine the current directory path with the CSV file name to get the full file path\n",
    "dataset_path = os.path.join(current_directory, csv_filename)\n",
    "\n",
    "chunk_size = 10000  # You can adjust this size based on your memory availability\n",
    "chunks = pd.read_csv(dataset_path, chunksize=chunk_size)\n",
    "\n",
    "# Load city data\n",
    "city_dataset_path = os.path.join(current_directory, \"city-table.csv\")\n",
    "city_df = pd.read_csv(city_dataset_path)\n",
    "city_df.rename(columns={'id': 'city_id'}, inplace=True)\n",
    "\n",
    "# Normalize city and state columns in city_df\n",
    "city_df['city'] = city_df['city'].str.strip().str.lower()\n",
    "city_df['state'] = city_df['state'].str.strip().str.lower()\n",
    "\n",
    "# Initialize a flag to indicate the first chunk\n",
    "is_first_chunk = True\n",
    "\n",
    "# Process each chunk\n",
    "for chunk in chunks:\n",
    "    # Normalize city and state columns in each chunk\n",
    "    chunk['city'] = chunk['city'].str.strip().str.lower()\n",
    "    chunk['state'] = chunk['state'].str.strip().str.lower()\n",
    "    \n",
    "    merged_chunk = pd.merge(chunk, city_df, on=['city', 'state'], how='left')\n",
    "    \n",
    "    # Check for any entries that did not get a city_id (these will need to be addressed)\n",
    "    if merged_chunk['city_id'].isnull().any():\n",
    "        print(\"Some entries did not get a city_id. Review the unmatched city and state names.\")\n",
    "    \n",
    "    merged_chunk.drop(columns=['city', 'state'], inplace=True)\n",
    "    # Append each processed chunk to a new CSV file\n",
    "    merged_chunk.to_csv('updated_real-estate-listings.csv', mode='a', index=False, header=is_first_chunk)  # Only write header for first chunk\n",
    "    is_first_chunk = False  # After the first chunk, set the flag to False\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
